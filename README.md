#  Academic Machine Learning (AKML) Project

## The project

The main goal of this project is to build from scratch and with the help of as less as possible external libraries and ressources neural networks that are lite, powerfull, and fully understandable. 

For now, this project has as goal to provide an implementation of a MultiLayer Perceptron with a Genetic algorithm and a gradient descent. 

The main project is written in C++ with the standard library and without any dynamic container (no std::list or std::vector in the core of the neural network). Hence, the code is harsh, and an absolute nightmare to read and use. Nonetheless, this constraint allows us to better follow the maths behind the project (matrix are by definition statics arrays... no matter how much programmers would like the contrary) and to be very memory-efficient.  


## Stage of development 

For now, the main core of the project is being written. A documentation and a refactoring (with namespaces for instance) is planned. 
